endpoint
for (temp.hour in 1:84) {
# this is the hour we are extracting.
print(temp.hour)
curr.hour = str_pad(temp.hour, 3, pad = "0")
curr.month = str_pad(month(current.dt), 2, pad = "0")
curr.day = str_pad(day(current.dt), 2, pad = "0")
# URL
endpoint = paste(
"https://dd.weather.gc.ca/model_gem_regional/10km/grib2/00/",
curr.hour,
"/CMC_reg_ACPCP_SFC_0_ps10km_",
year(current.dt),
curr.month,
curr.day,
HH,
"_P",
curr.hour,
".grib2",
sep = ""
)
# print(endpoint)
# # get the file
poll.response <- GET(endpoint)
#
# # give the tif a name
poll.name = paste("forecast_tmp",
"_Base",
HH,
"_",
formatted.date,
"_H",
curr.hour,
".tif",
sep = "")
#
# # save the raster
writeBin(content(poll.response, "raw"), poll.name)
# import the raster
temp.ras = raster(poll.name)
# extract values at smoke communities
pm = raster::extract(temp.ras, smoke.communities)
# add to a data frame
df[[curr.hour]] <- pm
# delete the raster file
file.remove(poll.name)
}
smoke_tz = read.csv("/Users/priyapatel/Library/CloudStorage/OneDrive-UniversityofToronto/!CANUE/AQHI Work/SmoKE_ID_communities_w_TZ.csv")
# More information on Canadian time zones : https://www.timeanddate.com/time/zone/canada
combined = cbind(smoke_tz,df)
combined <- subset(combined, select=-c(lat,lon))
# filter
unique(combined$name)
ET = combined[(combined$name == 'ET'),]
et.day1 = ET[,"006":"029"]
et.day1.max = apply(et.day1, 1, max)
##### day 2 #####
et.day2 = ET[,"030":"053"]
et.day2.max = apply(et.day2, 1, max)
##### day 3 #####
et.day3 = ET[,"054":"081"]
et.day3.max = apply(et.day3, 1, max)
et.day1.max = apply(et.day1, 1, sum)
et.day1.max
et.day2 = ET[,"030":"053"]
et.day2.max = apply(et.day2, 1, max)
et.day2.max
et.day3 = ET[,"054":"081"]
et.day3.max = apply(et.day3, 1, max)
et.day3.max
combined
# this is the hour we are extracting.
print(temp.hour)
curr.hour = str_pad(temp.hour, 3, pad = "0")
curr.month = str_pad(month(current.dt), 2, pad = "0")
curr.day = str_pad(day(current.dt), 2, pad = "0")
# URL
endpoint = paste(
"https://dd.weather.gc.ca/model_gem_regional/10km/grib2/00/",
curr.hour,
"/CMC_reg_ACPCP_SFC_0_ps10km_",
year(current.dt),
curr.month,
curr.day,
HH,
"_P",
curr.hour,
".grib2",
sep = ""
)
# # get the file
poll.response <- GET(endpoint)
poll.response
#
# # give the tif a name
poll.name = paste("forecast_tmp",
"_Base",
HH,
"_",
formatted.date,
"_H",
curr.hour,
".tif",
sep = "")
#
# # save the raster
writeBin(content(poll.response, "raw"), poll.name)
# import the raster
temp.ras = raster(poll.name)
temp.ras
temp.ras$SFC.Ground.or.water.surface..Convective.precipitation..kg..m.2..
plot(temp.ras)
# extract values at smoke communities
pm = raster::extract(temp.ras, smoke.communities)
pm
wd = dirname(rstudioapi::getSourceEditorContext()$path)
setwd(wd)
## this script is to get the current firework PM2.5
##### import libraries #####
library(httr)
library(raster)
library(sf)
library(tidyverse)
library(rvest)
##### import shapefile  #####
smoke.communities = "/Users/priyapatel/Library/CloudStorage/OneDrive-UniversityofToronto/!CANUE/AQHI Work/shapefiles/smoke_shp/smoke.shp"
smoke.communities = st_read(smoke.communities)
# get the current time and date
current.time = as.POSIXlt(Sys.time(), tz = "UTC")
hour(current.time)
##### these are the dates of interest
# get the current hour and date
date_nearest.hour = floor_date(current.time, "hour")
#### this is the file format
# https://dd.weather.gc.ca/model_rdaqa/10km/00/20231006T00Z_MSC_RDAQA-FW_PM2.5_Sfc_RLatLon0.09_PT0H.grib2
########## GET THE RASTERS  ###########
month.current = str_pad(month(date_nearest.hour), 2, pad = "0")
day.current = str_pad(day(current.time), 2, pad = "0")
hour.current = str_pad((hour(current.time)-2), 2, pad = "0")
# create a URL
# https://dd.weather.gc.ca/model_rdaqa/10km/22/20240109T22Z_MSC_RDAQA-FW_PM2.5_Sfc_RLatLon0.09_PT0H.grib2
# https://dd.weather.gc.ca/model_rdaqa/10km/22/20240110T22Z_MSC_RDAQA-FW_PM2.5_Sfc_RLatLon0.09_PT0H.grib2"
endpoint.url = paste(
"https://dd.weather.gc.ca/model_rdaqa/10km/",
hour.current,
"/",
year(date_nearest.hour),
month.current,
day.current,
"T",
hour.current,
"Z_MSC_RDAQA-FW_PM2.5_Sfc_RLatLon0.09_PT0H.grib2",
sep = ""
)
# get the file
poll.response <- GET(endpoint.url)
# give the tif a name
poll.name = paste("FW_PM2.5", "_", date(current.time), "_H", hour(current.time), ".tif", sep =
"")
# save the raster
writeBin(content(poll.response, "raw"), poll.name)
pm25 = raster(poll.name)
## extract values from rasters at the smoke communities
pm25_smoke = raster::extract(pm25,smoke.communities)*1000000000
combined = cbind(smoke.communities %>% st_drop_geometry(),pm25_smoke)
write.csv(combined,"current_aqhi.csv")
write.csv(combined,"current_fw.csv")
# set the working directory
wd = dirname(rstudioapi::getSourceEditorContext()$path)
setwd(wd)
library(lubridate)
library(stringr)
library(httr)
library(raster)
library(sf)
library(tidyverse)
library(rvest)
#### import smoke data ####
smoke.communities = "/Users/priyapatel/Library/CloudStorage/OneDrive-UniversityofToronto/!CANUE/AQHI Work/shapefiles/smoke_shp/smoke.shp"
smoke.communities = st_read(smoke.communities)
# info on the dataset: https://eccc-msc.github.io/open-data/msc-data/nwp_raqdps/readme_raqdps-datamart_en/
# https://dd.weather.gc.ca/model_raqdps-fw/10km/grib2/00/000/20240109T00Z_MSC_RAQDPS-FW_PM2.5_Sfc_RLatLon0.09_PT000H.grib2
HH = "00"
current.time = as.POSIXlt(Sys.time(), tz = "UTC")
formatted.date = format(current.time, "%Y%m%d")
################ 1. Download all pollutant files for 72 hours. ################
VAR = "PM2.5"
df <- data.frame(ID = 1:698)
for (temp.hour in 0:72) {
# this is the hour we are extracting.
print(temp.hour)
curr.hour = str_pad(temp.hour, 3, pad = "0")
# # get the URL
endpoint = paste(
"https://dd.weather.gc.ca/model_raqdps-fw/10km/grib2/",
HH,
"/",
curr.hour,
"/",
formatted.date,
"T",
HH,
"Z_MSC_RAQDPS-FW_PM2.5_Sfc_RLatLon0.09_PT",
curr.hour,
"H.grib2",
sep = ""
)
# print(endpoint)
# # get the file
poll.response <- GET(endpoint)
#
# # give the tif a name
poll.name = paste("forecast_",
VAR,
"_Base",
HH,
"_",
formatted.date,
"_H",
curr.hour,
".tif",
sep = "")
#
# # save the raster
writeBin(content(poll.response, "raw"), poll.name)
# import the raster
temp.ras = raster(poll.name) * 1000000000
# extract values at smoke communities
pm = raster::extract(temp.ras, smoke.communities)
# add to a data frame
df[[curr.hour]] <- pm
# delete the raster file
file.remove(poll.name)
}
################# calculate three hour average for NO2. ################
# import data on smoke communities and the associated time zone
smoke_tz = read.csv("/Users/priyapatel/Library/CloudStorage/OneDrive-UniversityofToronto/!CANUE/AQHI Work/SmoKE_ID_communities_w_TZ.csv")
# More information on Canadian time zones : https://www.timeanddate.com/time/zone/canada
combined = cbind(smoke_tz,df)
combined <- subset(combined, select=-c(lat,lon))
# filter
unique(combined$name)
####################################################################################
#### eastern and atlantic time zones: ####
##### day 1 #####
ET = combined[(combined$name == 'ET'),]
et.day1 = ET[,"006":"029"]
et.day1.max = apply(et.day1, 1, max)
##### day 2 #####
et.day2 = ET[,"030":"053"]
et.day2.max = apply(et.day2, 1, max)
##### day 3 #####
et.day3 = ET[,"054":"073"]
et.day3.max = apply(et.day3, 1, max)
######################
#### pacific time zone: ####
##### day 1 #####
PT = combined[(combined$name == 'PT'),]
pt.day1 = PT[,"009":"032"]
pt.day1.max = apply(pt.day1, 1, max)
##### day 2 #####
pt.day2 = PT[,"033":"056"]
pt.day2.max = apply(pt.day2, 1, max)
##### day 3 #####
pt.day3 = PT[,"057":"073"]
pt.day3.max = apply(pt.day3, 1, max)
#### mountain standard time: ####
##### day 1 #####
MT = combined[(combined$name == 'MT'),]
mt.day1 = MT[,"008":"031"]
mt.day1.max = apply(mt.day1, 1, max)
##### day 2 #####
mt.day2 = MT[,"032":"055"]
mt.day2.max = apply(mt.day2, 1, max)
##### day 3 #####
mt.day3 = MT[,"055":"073"]
mt.day3.max = apply(mt.day3, 1, max)
#### central standard time: ####
##### day 1 #####
CT = combined[(combined$name == 'CT'),]
ct.day1 = CT[,"007":"030"]
ct.day1.max = apply(ct.day1, 1, max)
##### day 2 #####
ct.day2 = CT[,"031":"054"]
ct.day2.max = apply(ct.day2, 1, max)
##### day 3 #####
ct.day3 = CT[,"055":"073"]
ct.day3.max = apply(ct.day3, 1, max)
#### atlantic standard time: ####
##### day 1 #####
AT = combined[(combined$name == 'AT'| combined$name =="NT"),]
at.day1 = AT[,"005":"028"]
at.day1.max = apply(at.day1, 1, max)
##### day 2 #####
at.day2 = AT[,"029":"052"]
at.day2.max = apply(at.day2, 1, max)
##### day 3 #####
at.day3 = AT[,"053":"073"]
at.day3.max = apply(at.day3, 1, max)
# combine all the data
day1.combine = as.data.frame(c(ct.day1.max, et.day1.max, pt.day1.max, mt.day1.max,at.day1.max))
day2.combine = as.data.frame(c(ct.day2.max, et.day2.max, pt.day2.max, mt.day2.max,at.day2.max))
day3.combine = as.data.frame(c(ct.day3.max, et.day3.max, pt.day3.max, mt.day3.max,at.day3.max))
day1.combine = day1.combine[order(as.numeric(rownames(day1.combine))),,drop=FALSE]
day2.combine = day2.combine[order(as.numeric(rownames(day2.combine))),,drop=FALSE]
day3.combine = day3.combine[order(as.numeric(rownames(day3.combine))),,drop=FALSE]
#  limit decimal places
options(digits=2)
# combine in the table
df <- data.frame(ID = 1:698, day1.combine,day2.combine,day3.combine)
colnames(df)[2] <- "currentmax"
colnames(df)[3] <- "day2max"
colnames(df)[4] <- "day3max"
write.csv(df,"maxvalues.csv",row.names = FALSE)
## this script is to get the current precipitation
# set working directory
wd = dirname(rstudioapi::getSourceEditorContext()$path)
setwd(wd)
# Data is coming from here:  https://eccc-msc.github.io/open-data/msc-data/nwp_rdps/readme_rdps-datamart_en/
#### this is the file format : CMC_reg_Variable_LevelType_level_ps10km_YYYYMMDDHH_Phhh.grib2
##### import libraries #####
library(httr)
library(raster)
library(sf)
library(tidyverse)
library(rvest)
##### import shapefile  #####
smoke.communities = "/Users/priyapatel/Library/CloudStorage/OneDrive-UniversityofToronto/!CANUE/AQHI Work/shapefiles/smoke_shp/smoke.shp"
smoke.communities = st_read(smoke.communities)
# get the current time and date
current.dt = as.POSIXlt(Sys.time(), tz = "UTC")
##### these are the dates of interest
# get the current hour and date
date_nearest.hour = floor_date(current.dt, "hour")
########## GET THE RASTERS  ###########
month.current = str_pad(month(current.dt), 2, pad = "0")
day.current = str_pad(day(current.dt), 2, pad = "0")
hour.current = str_pad((hour(date_nearest.hour)), 3, pad = "0")
HH = "00"
# create a URL
endpoint = paste(
"https://dd.weather.gc.ca/model_gem_regional/10km/grib2/00/",
hour.current,
"/CMC_reg_ACPCP_SFC_0_ps10km_",
year(current.dt),
month.current,
day.current,
HH,
"_P",
hour.current,
".grib2",
sep = ""
)
# get the file
poll.response <- GET(endpoint)
# give the tif a name
poll.name = paste("TMP", "_", date(current.dt), "_H", hour(current.dt), ".tif", sep =
"")
# save the raster
writeBin(content(poll.response, "raw"), poll.name)
# create a raster
tmp = raster(poll.name)
## extract values from rasters at the smoke communities
tmp_smoke = raster::extract(tmp,smoke.communities)
combined = cbind(smoke.communities %>% st_drop_geometry(),tmp_smoke)
write.csv(combined,"current_precip.csv")
# set the working directory
wd = dirname(rstudioapi::getSourceEditorContext()$path)
setwd(wd)
library(lubridate)
library(stringr)
library(httr)
library(raster)
library(sf)
library(tidyverse)
library(rvest)
# Data is coming from here:  https://eccc-msc.github.io/open-data/msc-data/nwp_rdps/readme_rdps-datamart_en/
#### this is the file format : CMC_reg_Variable_LevelType_level_ps10km_YYYYMMDDHH_Phhh.grib2
#### import smoke data ####
smoke.communities = "/Users/priyapatel/Library/CloudStorage/OneDrive-UniversityofToronto/!CANUE/AQHI Work/shapefiles/smoke_shp/smoke.shp"
smoke.communities = st_read(smoke.communities)
HH = "00"
current.dt = as.POSIXlt(Sys.time(), tz = "UTC")
formatted.date = format(current.dt, "%Y%m%d")
################ 1. Download all pollutant files for 72 hours. ################
df <- data.frame(ID = 1:698)
for (temp.hour in 1:84) {
# this is the hour we are extracting.
print(temp.hour)
curr.hour = str_pad(temp.hour, 3, pad = "0")
curr.month = str_pad(month(current.dt), 2, pad = "0")
curr.day = str_pad(day(current.dt), 2, pad = "0")
# URL
endpoint = paste(
"https://dd.weather.gc.ca/model_gem_regional/10km/grib2/00/",
curr.hour,
"/CMC_reg_ACPCP_SFC_0_ps10km_",
year(current.dt),
curr.month,
curr.day,
HH,
"_P",
curr.hour,
".grib2",
sep = ""
)
# print(endpoint)
# # get the file
poll.response <- GET(endpoint)
#
# # give the tif a name
poll.name = paste("forecast_tmp",
"_Base",
HH,
"_",
formatted.date,
"_H",
curr.hour,
".tif",
sep = "")
#
# # save the raster
writeBin(content(poll.response, "raw"), poll.name)
# import the raster
temp.ras = raster(poll.name)
# extract values at smoke communities
pm = raster::extract(temp.ras, smoke.communities)
# add to a data frame
df[[curr.hour]] <- pm
# delete the raster file
file.remove(poll.name)
}
################# calculate three hour average for NO2. ################
# import data on smoke communities and the associated time zone
smoke_tz = read.csv("/Users/priyapatel/Library/CloudStorage/OneDrive-UniversityofToronto/!CANUE/AQHI Work/SmoKE_ID_communities_w_TZ.csv")
# More information on Canadian time zones : https://www.timeanddate.com/time/zone/canada
combined = cbind(smoke_tz,df)
combined <- subset(combined, select=-c(lat,lon))
# filter
unique(combined$name)
####################################################################################
#### eastern and atlantic time zones: ####
##### day 1 #####
ET = combined[(combined$name == 'ET'),]
et.day1 = ET[,"006":"029"]
et.day1.sum = apply(et.day1, 1, sum)
##### day 2 #####
et.day2 = ET[,"030":"053"]
et.day2.sum = apply(et.day2, 1, sum)
##### day 3 #####
et.day3 = ET[,"054":"081"]
et.day3.sum = apply(et.day3, 1, sum)
######################
#### pacific time zone: ####
##### day 1 #####
PT = combined[(combined$name == 'PT'),]
pt.day1 = PT[,"009":"032"]
pt.day1.sum = apply(pt.day1, 1, sum)
##### day 2 #####
pt.day2 = PT[,"033":"056"]
pt.day2.sum = apply(pt.day2, 1, sum)
##### day 3 #####
pt.day3 = PT[,"057":"084"]
pt.day3.sum = apply(pt.day3, 1, sum)
#### mountain standard time: ####
##### day 1 #####
MT = combined[(combined$name == 'MT'),]
mt.day1 = MT[,"008":"031"]
mt.day1.sum = apply(mt.day1, 1, sum)
##### day 2 #####
mt.day2 = MT[,"032":"055"]
mt.day2.sum = apply(mt.day2, 1, sum)
##### day 3 #####
mt.day3 = MT[,"055":"083"]
mt.day3.sum = apply(mt.day3, 1, sum)
#### central standard time: ####
##### day 1 #####
CT = combined[(combined$name == 'CT'),]
ct.day1 = CT[,"007":"030"]
ct.day1.sum = apply(ct.day1, 1, sum)
##### day 2 #####
ct.day2 = CT[,"031":"054"]
ct.day2.sum = apply(ct.day2, 1, sum)
##### day 3 #####
ct.day3 = CT[,"055":"082"]
ct.day3.sum = apply(ct.day3, 1, sum)
#### atlantic standard time: ####
##### day 1 #####
AT = combined[(combined$name == 'AT'| combined$name =="NT"),]
at.day1 = AT[,"005":"028"]
at.day1.sum = apply(at.day1, 1, sum)
##### day 2 #####
at.day2 = AT[,"029":"052"]
at.day2.sum = apply(at.day2, 1, sum)
##### day 3 #####
at.day3 = AT[,"053":"080"]
at.day3.sum = apply(at.day3, 1, sum)
# combine all the data
day1.combine = as.data.frame(c(ct.day1.sum, et.day1.sum, pt.day1.sum, mt.day1.sum,at.day1.sum))
day2.combine = as.data.frame(c(ct.day2.sum, et.day2.sum, pt.day2.sum, mt.day2.sum,at.day2.sum))
day3.combine = as.data.frame(c(ct.day3.sum, et.day3.sum, pt.day3.sum, mt.day3.sum,at.day3.sum))
day1.combine = day1.combine[order(as.numeric(rownames(day1.combine))),,drop=FALSE]
day2.combine = day2.combine[order(as.numeric(rownames(day2.combine))),,drop=FALSE]
day3.combine = day3.combine[order(as.numeric(rownames(day3.combine))),,drop=FALSE]
#  limit decimal places
options(digits=2)
# combine in the table
df <- data.frame(ID = 1:698, day1.combine,day2.combine,day3.combine)
colnames(df)[2] <- "currentsum"
colnames(df)[3] <- "day2sum"
colnames(df)[4] <- "day3sum"
write.csv(df,"sumvalues.csv",row.names = FALSE)
